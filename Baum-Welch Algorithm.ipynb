{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baum-Welch Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm is using the EM algorithm for Hidden Markov model, our goal is the get the transition probability and emission probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous algorithm\n",
    "def forward(V, a, b, initial_distribution):\n",
    "    ####################################################\n",
    "    # V : it is the data for visible state             #\n",
    "    # a : transition probability                       #\n",
    "    # b : emission probability                         #\n",
    "    # initial_distribution : start that you assume     #\n",
    "    ####################################################\n",
    "    alpha = np.zeros((V.shape[0], a.shape[0]))        # 500 X 2\n",
    "    alpha[0, :] = initial_distribution * b[:, V[0]]   # first row = initial * first visible data\n",
    " \n",
    "    for t in range(1, V.shape[0]):\n",
    "        for j in range(a.shape[0]):\n",
    "            alpha[t, j] = alpha[t - 1].dot(a[:, j]) * b[j, V[t]]\n",
    " \n",
    "    return alpha\n",
    "\n",
    "def backward(V, a, b):\n",
    "    ###################################################################################\n",
    "    # V : it is the data for visible state                                            #\n",
    "    # a : transition probability                                                      #\n",
    "    # b : emission probability                                                        #\n",
    "    # backward start from assign each state 1, so we don't need initial probability.  #\n",
    "    ###################################################################################\n",
    "    beta = np.zeros((V.shape[0], a.shape[0]))\n",
    " \n",
    "    # setting beta(T) = 1\n",
    "    beta[V.shape[0] - 1] = np.ones((a.shape[0]))\n",
    " \n",
    "    # Loop in backward way from T-1 to\n",
    "    # Due to python indexing the actual loop will be T-2 to 0\n",
    "    for t in range(V.shape[0] - 2, -1, -1):\n",
    "        for j in range(a.shape[0]):\n",
    "            beta[t, j] = (beta[t + 1] * b[:, V[t + 1]]).dot(a[j, :])\n",
    " \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first start with guessing the result of transition probability and emission probability, also the proportion of the hidden state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data for HMM.csv')\n",
    " \n",
    "V = data['Visible'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to get the estimation of proportion of hidden state, transition probability and emission probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simply assume the proportion to be the same (0.5, 0.5), and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "[[0.11111111 0.33333333 0.55555556]\n",
      " [0.16666667 0.33333333 0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "# it's just a initial guess, we will get the precise transition probability and emission probability by the algorithm\n",
    "a = np.ones((2, 2))\n",
    "a = a / np.sum(a, axis=1)\n",
    "print(a)\n",
    "b = np.array(((1, 3, 5), (2, 4, 6)))\n",
    "b = b / np.sum(b, axis=1).reshape((-1, 1))\n",
    "print(b)\n",
    "initial_distribution = np.array((0.5, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baum_welch(V, a, b, initial_distribution, n_iter=100):\n",
    "    M = a.shape[0]\n",
    "    T = len(V)\n",
    " \n",
    "    for n in range(n_iter):\n",
    "        alpha = forward(V, a, b, initial_distribution)\n",
    "        beta = backward(V, a, b)\n",
    " \n",
    "        xi = np.zeros((M, M, T - 1))\n",
    "        for t in range(T - 1):\n",
    "            denominator = np.dot(np.dot(alpha[t, :].T, a) * b[:, V[t + 1]].T, beta[t + 1, :])\n",
    "            for i in range(M):\n",
    "                numerator = alpha[t, i] * a[i, :] * b[:, V[t + 1]].T * beta[t + 1, :].T\n",
    "                xi[i, :, t] = numerator / denominator\n",
    " \n",
    "        gamma = np.sum(xi, axis=1)\n",
    "        #print(np.sum(gamma, axis = 1))\n",
    "        v = np.sum(gamma, axis = 1)/T\n",
    "        a = np.sum(xi, 2) / np.sum(gamma, axis=1).reshape((-1, 1))\n",
    " \n",
    "        # Add additional T'th element in gamma\n",
    "        gamma = np.hstack((gamma, np.sum(xi[:, :, T - 2], axis=0).reshape((-1, 1))))\n",
    " \n",
    "        K = b.shape[1]\n",
    "        denominator = np.sum(gamma, axis=1)\n",
    "        for l in range(K):\n",
    "            b[:, l] = np.sum(gamma[:, V == l], axis=1)\n",
    " \n",
    "        b = np.divide(b, denominator.reshape((-1, 1)))\n",
    " \n",
    "    return {\"a\":a, \"b\":b, \"initial_distribution\": v}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': array([[0.49354007, 0.50645993],\n",
      "       [0.49321683, 0.50678317]]), 'b': array([[0.16707575, 0.27372847, 0.55919578],\n",
      "       [0.24387812, 0.26637174, 0.48975014]]), 'initial_distribution': array([0.49213684, 0.50586316])}\n"
     ]
    }
   ],
   "source": [
    "a = np.ones((2, 2))\n",
    "a = a / np.sum(a, axis=1)\n",
    "\n",
    "b = np.array(((1, 3, 5), (2, 4, 6)))\n",
    "b = b / np.sum(b, axis=1).reshape((-1, 1))\n",
    "\n",
    "initial_distribution = np.array((0.5, 0.5))\n",
    "\n",
    "result = baum_welch(V, a, b, initial_distribution, n_iter=1)\n",
    "print(result)   # I add the initial probability part, but I'm not sure the answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I want to revise the algorithm to use the $\\epsilon$ to be the condition to stop, but it stop at a weird place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baum_welch2(V, a, b, initial_distribution, epi = 0.0001):\n",
    "    M = a.shape[0]\n",
    "    T = len(V)\n",
    "    b_org = b.copy()\n",
    "    \n",
    "    while True:\n",
    "        alpha = forward(V, a, b, initial_distribution)\n",
    "        beta = backward(V, a, b)\n",
    "        \n",
    " \n",
    "        xi = np.zeros((M, M, T - 1))\n",
    "        for t in range(T - 1):\n",
    "            denominator = np.dot(np.dot(alpha[t, :].T, a) * b[:, V[t + 1]].T, beta[t + 1, :])\n",
    "            for i in range(M):\n",
    "                numerator = alpha[t, i] * a[i, :] * b[:, V[t + 1]].T * beta[t + 1, :].T\n",
    "                xi[i, :, t] = numerator / denominator\n",
    "        \n",
    "        gamma = np.sum(xi, axis=1)\n",
    "        a_new = np.sum(xi, 2) / np.sum(gamma, axis=1).reshape((-1, 1))\n",
    " \n",
    "        # Add additional T'th element in gamma\n",
    "        gamma = np.hstack((gamma, np.sum(xi[:, :, T - 2], axis=0).reshape((-1, 1))))\n",
    "        \n",
    "        K = b.shape[1]\n",
    "        denominator = np.sum(gamma, axis=1)\n",
    "        for l in range(K):\n",
    "            b[:, l] = np.sum(gamma[:, V == l], axis=1)\n",
    " \n",
    "        b_new = np.divide(b, denominator.reshape((-1, 1)))\n",
    "        \n",
    "        # check the difference is small enouth to break\n",
    "        #a_diff = np.array(a) - np.array(a_new)     # try to use norm 2 to compare, but weird\n",
    "        #b_diff = np.array(b_org) - np.array(b_new)\n",
    "        #if np.linalg.norm(a_diff, 2) + np.linalg.norm(b_diff, 2) < epi:\n",
    "        #    break\n",
    "        #else:\n",
    "        #    a = a_new\n",
    "        #    b = b_new\n",
    "        #    b_org = b.copy()\n",
    "        alpha_2 = forward(V, a_new, b_new, initial_distribution)  # use the likelihood to compare\n",
    "        if abs(alpha[-1].sum() - alpha_2[-1].sum()) < epi:\n",
    "            print(abs(alpha[-1].sum() - alpha_2[-1].sum()))\n",
    "            break\n",
    "        else:\n",
    "            a = a_new\n",
    "            b = b_new\n",
    "            b_org = b.copy()\n",
    "\n",
    "    return {\"a\":a_new, \"b\":b_new}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3044550522455806e-225\n",
      "{'a': array([[0.49388933, 0.50611067],\n",
      "       [0.49324199, 0.50675801]]), 'b': array([[0.16710908, 0.27371913, 0.55917179],\n",
      "       [0.24387565, 0.26637796, 0.48974639]])}\n"
     ]
    }
   ],
   "source": [
    "a = np.ones((2, 2))\n",
    "a = a / np.sum(a, axis=1)\n",
    "\n",
    "b = np.array(((1, 3, 5), (2, 4, 6)))\n",
    "b = b / np.sum(b, axis=1).reshape((-1, 1))\n",
    "\n",
    "initial_distribution = np.array((0.5, 0.5))\n",
    "\n",
    "result2 = baum_welch2(V, a, b, initial_distribution, 10**(-221))  # the criterior is weird, \n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference : \n",
    "*http://www.adeveloperdiary.com/data-science/machine-learning/derivation-and-implementation-of-baum-welch-algorithm-for-hidden-markov-model/* --> gives me the idea about how to implement it\n",
    "\n",
    "*https://github.com/hamzarawal/HMM-Baum-Welch-Algorithm/blob/2c5883c42f171f3c54a9afa4ae85eeabb196dfde/baum-welch.py#L110* --> gives me the idea about using likelihood to compare ! but its data is small, so when data goes bigger, the criterior goes crazy ! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
